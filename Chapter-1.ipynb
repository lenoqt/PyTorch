{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "tensor([[0.1230, 0.2047],\n",
      "        [0.9670, 0.1555]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.rand(2,2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tensor is a container for number as well as a set of rules that define transformations between tensors that produce new tensors -> Multidimensional arrays. Every tensor has a rank, i.e. An scalar (e.g., 1) can be represented as a tensor of rank 0, a vector is rank 1, an ${n}x{n}$ matrix is rank 2, and so on. The previous example we created a rank 2 tensor with random values ${row_n} x {cols_n}$ using ```torch.rand()```\n",
    "\n",
    "e.g.,\n",
    "\n",
    "\n",
    "\n",
    "![title](https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fwww.machinecurve.com%2Fwp-content%2Fuploads%2F2020%2F04%2Frankshape.png&f=1&nofb=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[0,0,1],[1,1,1], [0,0,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 1],\n",
       "        [1, 1, 1],\n",
       "        [0, 0, 0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can change a element in the tensor by using Python indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0][0] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5, 0, 1],\n",
       "        [1, 1, 1],\n",
       "        [0, 0, 0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Special creation functions can be use to generate particular types of tensors. in particular ```ones()``` and ```zeroes()``` wil generate tensors filled with ${0s}$ and ${1s}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(2,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(2,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can perform standard mathematical operation with tensors (e.g., adding two tensors together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(1,2) + torch.ones(1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if you have a tensor of rank 0, you can pull out the value with ```item()``` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1727508306503296"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(1).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors can live in the CPU or on the GPU and can be copied between devices by using the ```to()``` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpu_tensor = torch.rand(2)\n",
    "cpu_tensor.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_tensor = cpu_tensor.to('cuda')\n",
    "gpu_tensor.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tensor Operations**\n",
    "\n",
    "Looking at [PyTorch Documentation](https://pytorch.org/docs/stable/index.html), there are a lot of functions applicable to tensors.\n",
    "\n",
    "To find the maximun item in a tensor as well as the index that contains the maximun value (as this often corresponds to the class that the neural network has decided upon its final prediction). These can be done with ```max()``` and ```argmax()``` functions. We can also use ```item()``` to extract a standard Python value from a 1D tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8735)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(2,2).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8800461888313293"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(2,2).max().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To change the type of a tensor; for example, from ```LongTensor``` to a ```FloatTensor``` use  ```to()``` method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_tensor = torch.tensor([[0,0,1], [1,1,1], [0,0,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.LongTensor'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_tensor.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_tensor = torch.tensor([[0,0,1], [1,1,1], [0,0,0]]).to(dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float_tensor.type()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most functions that operate on a tensor and return a tensor create a new tensor to store the result. However, if you want to save memory, look to see if an ```in-place``` function is defined, which should be the same name as the original function but with an appended underscore ```(_)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_tensor = torch.rand(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3776, -0.1421],\n",
       "        [-0.0483, -0.7601]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_tensor.log2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3776, -0.1421],\n",
       "        [-0.0483, -0.7601]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_tensor.log2_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common operation is ```reshape()``` of a tensor. This can often occur because your neural network layer may require a slightly different input shape than what you currently have to feed into it. For example, the MNIST dataset of handwritten digits is a collection of 28x28 images, but the way it's packaged is in arrays of lenght 784. To use the networks we are construction, we need to turn those back into 1x28x28 tensors (the leading 1 is the number of channels--normally red, green and blue-- but as MNIST digits are just grayscale, we have only one channel). We can do this with either ```view()``` or ```reshape()```\n",
    "\n",
    "e.g., \n",
    "\n",
    "$$\n",
    "\\mathbf{flat}\n",
    "=>\n",
    "flat[{x_0},{x_1},{x_2},{x_3}\\dots\\,{x_n},\\dots\\,{x_{784}}]\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "{x_{0,0}} & \\cdots &\n",
    "{x_{0,28}} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "{x_{28,0}} & \\cdots &\n",
    "{x_{28,28}}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_tensor = torch.rand(784) #28*28 = 784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewed_tensor = flat_tensor.view(1,28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viewed_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_tensor = flat_tensor.reshape(1,28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the reshaped tensor's shape has to have the same number of total elements as the original. if you try ```flat_tensor.reshape(3,28,28)``` you'll see and error like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[3, 28, 28]' is invalid for input of size 784",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-774c70ba5c08>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mflat_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[3, 28, 28]' is invalid for input of size 784"
     ]
    }
   ],
   "source": [
    "flat_tensor.reshape(3,28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.5889e-02, 6.5575e-01, 9.4044e-01, 8.6964e-01, 8.8507e-01, 6.8216e-01,\n",
       "        9.5707e-01, 2.7690e-01, 8.8204e-01, 9.5755e-01, 2.3647e-01, 4.2960e-01,\n",
       "        6.6124e-01, 1.1521e-01, 2.1228e-01, 1.0569e-01, 1.4606e-01, 8.6432e-01,\n",
       "        8.0503e-01, 9.1108e-01, 5.8659e-02, 2.0922e-01, 4.8596e-01, 2.0913e-02,\n",
       "        6.1477e-01, 3.5737e-01, 7.3118e-02, 2.7390e-01, 5.3700e-01, 9.9846e-01,\n",
       "        4.8384e-01, 1.8647e-01, 6.4972e-01, 2.1345e-01, 7.5897e-01, 2.8915e-01,\n",
       "        8.0595e-01, 8.9152e-01, 3.8298e-01, 3.1125e-01, 5.5853e-01, 1.3844e-01,\n",
       "        8.4187e-01, 8.2524e-01, 7.7212e-01, 2.4335e-02, 6.9185e-01, 5.9948e-01,\n",
       "        3.0699e-01, 6.5634e-01, 7.6257e-01, 4.3992e-01, 6.5125e-02, 7.1936e-01,\n",
       "        6.4578e-01, 5.6842e-01, 8.5521e-01, 7.6863e-01, 6.2816e-01, 8.8526e-01,\n",
       "        2.2754e-02, 6.6599e-01, 9.7909e-01, 4.9220e-01, 2.3924e-01, 3.9803e-01,\n",
       "        9.1217e-01, 8.8972e-01, 9.9680e-01, 5.1386e-01, 3.1144e-01, 4.5929e-01,\n",
       "        9.3372e-01, 3.8756e-01, 3.7463e-01, 9.6992e-01, 6.0018e-01, 7.8715e-01,\n",
       "        2.8794e-01, 4.7222e-01, 8.7738e-01, 4.3969e-01, 4.1539e-01, 6.0958e-04,\n",
       "        8.0455e-01, 3.6620e-02, 1.4184e-02, 3.3657e-01, 3.2987e-02, 2.9581e-01,\n",
       "        2.2468e-01, 3.6206e-01, 1.8273e-01, 1.4513e-01, 5.9439e-01, 9.4725e-01,\n",
       "        9.3905e-01, 8.8681e-01, 3.2379e-01, 2.7125e-01, 2.3617e-01, 5.3313e-01,\n",
       "        6.2630e-01, 7.2503e-01, 8.7898e-01, 8.9454e-01, 9.0990e-01, 2.6664e-01,\n",
       "        4.4340e-01, 3.4661e-01, 8.8873e-01, 6.6434e-01, 5.6167e-01, 4.1010e-01,\n",
       "        5.1526e-01, 3.1498e-02, 8.0325e-01, 9.6420e-01, 1.5970e-01, 7.8465e-02,\n",
       "        4.5734e-01, 9.1549e-01, 8.6436e-01, 5.8707e-01, 2.4061e-01, 9.0907e-01,\n",
       "        5.4250e-01, 2.9113e-01, 2.3601e-01, 7.8144e-01, 6.1498e-01, 1.3675e-01,\n",
       "        5.2027e-01, 4.6622e-01, 9.7079e-01, 8.9554e-01, 8.3824e-01, 3.2147e-01,\n",
       "        6.0413e-01, 6.3816e-02, 2.0835e-02, 6.5053e-01, 1.5440e-01, 5.0822e-01,\n",
       "        8.2654e-01, 9.3119e-01, 5.9145e-01, 7.6950e-01, 6.2876e-01, 1.7020e-02,\n",
       "        9.6169e-01, 7.2907e-02, 7.0624e-01, 3.0728e-01, 6.4361e-01, 2.9786e-01,\n",
       "        4.7886e-01, 8.7659e-01, 8.3370e-01, 1.6584e-01, 3.3076e-02, 2.0180e-01,\n",
       "        1.5283e-01, 9.8191e-01, 7.7700e-01, 6.5456e-01, 5.1807e-01, 3.3759e-01,\n",
       "        9.6436e-01, 3.3080e-01, 2.6057e-01, 8.1686e-01, 7.5143e-01, 5.9739e-01,\n",
       "        4.4254e-01, 7.4009e-02, 5.6592e-01, 7.3560e-02, 9.3016e-01, 1.2482e-01,\n",
       "        2.7788e-01, 5.7254e-01, 5.8000e-01, 5.7715e-01, 7.9643e-01, 1.1973e-01,\n",
       "        9.3500e-01, 6.9383e-01, 4.4302e-01, 7.8123e-01, 3.4865e-01, 8.0862e-01,\n",
       "        3.8910e-01, 2.4305e-01, 7.5497e-01, 1.3559e-01, 1.7011e-01, 2.7212e-01,\n",
       "        3.5032e-01, 5.7736e-01, 1.1592e-01, 7.7226e-01, 5.1269e-01, 6.3759e-01,\n",
       "        5.5798e-01, 9.4620e-02, 2.2569e-01, 4.5770e-01, 6.7660e-02, 7.3646e-01,\n",
       "        7.4057e-01, 9.0984e-01, 4.5274e-01, 6.1247e-01, 4.0446e-01, 6.8715e-01,\n",
       "        8.2896e-01, 8.5988e-01, 5.1877e-01, 8.2885e-01, 1.5109e-01, 4.4154e-01,\n",
       "        1.8012e-01, 4.7717e-01, 1.0407e-01, 3.2677e-01, 1.2609e-01, 2.5743e-01,\n",
       "        2.6130e-01, 2.3570e-01, 6.7067e-01, 2.8074e-01, 2.3857e-01, 1.6016e-01,\n",
       "        4.4210e-01, 3.9720e-01, 6.2891e-02, 9.7431e-01, 2.6979e-01, 8.3383e-01,\n",
       "        9.3715e-01, 2.4730e-01, 4.2970e-02, 1.1612e-01, 4.2779e-01, 9.7649e-01,\n",
       "        7.7159e-01, 1.5684e-01, 3.2371e-01, 5.4524e-01, 7.0595e-01, 1.2184e-02,\n",
       "        1.2130e-01, 4.6382e-01, 3.8554e-01, 8.4457e-01, 3.9757e-02, 4.0170e-01,\n",
       "        7.7603e-01, 7.0531e-01, 8.8525e-01, 1.2068e-01, 3.2927e-01, 6.6370e-01,\n",
       "        8.6914e-01, 2.4840e-01, 2.7883e-01, 4.1150e-01, 5.9454e-01, 9.1849e-01,\n",
       "        2.5452e-01, 9.5229e-01, 1.0500e-01, 8.6306e-01, 9.0709e-01, 7.4405e-01,\n",
       "        8.5814e-01, 8.7126e-01, 8.3599e-01, 7.7786e-01, 1.3348e-01, 1.9622e-01,\n",
       "        3.5093e-01, 8.3668e-01, 9.4728e-01, 5.7695e-01, 2.6075e-01, 5.7825e-01,\n",
       "        6.4812e-01, 1.8694e-01, 7.3879e-02, 2.6351e-01, 7.5954e-01, 8.8911e-01,\n",
       "        1.4033e-01, 8.9991e-01, 1.9517e-01, 1.9730e-01, 6.0839e-01, 6.1874e-01,\n",
       "        7.9717e-01, 5.3265e-01, 8.0858e-01, 6.4701e-01, 4.9098e-01, 2.0984e-01,\n",
       "        5.7899e-01, 9.2386e-01, 6.1324e-01, 5.6940e-01, 5.6068e-01, 5.4006e-01,\n",
       "        4.6195e-01, 2.3495e-01, 8.8660e-02, 5.1893e-01, 5.6282e-02, 3.2759e-01,\n",
       "        1.2530e-01, 2.1143e-01, 9.1163e-01, 3.8952e-01, 6.9121e-01, 8.0604e-01,\n",
       "        7.2835e-02, 1.8585e-01, 4.3062e-01, 3.3579e-01, 2.2949e-01, 4.9730e-01,\n",
       "        8.5033e-01, 9.0689e-01, 1.1663e-01, 7.9747e-01, 5.1266e-01, 5.8901e-01,\n",
       "        5.0285e-01, 8.7694e-01, 7.2519e-01, 3.6749e-02, 2.2775e-01, 7.0727e-01,\n",
       "        8.5279e-01, 2.4515e-01, 4.3678e-01, 6.4147e-01, 8.4006e-01, 4.1142e-01,\n",
       "        4.3410e-01, 7.0146e-01, 9.4380e-01, 8.3049e-01, 3.4416e-01, 5.7406e-01,\n",
       "        2.4620e-01, 9.5466e-01, 3.6933e-01, 6.4089e-01, 4.0808e-01, 1.1924e-01,\n",
       "        3.4321e-01, 7.2576e-01, 2.3323e-01, 1.5909e-01, 2.8431e-01, 1.6663e-01,\n",
       "        6.9221e-01, 3.1256e-01, 1.1712e-01, 9.2988e-01, 2.8665e-01, 5.5632e-01,\n",
       "        2.0839e-01, 6.0927e-01, 1.0395e-01, 9.5672e-01, 7.5815e-01, 9.5295e-01,\n",
       "        8.3029e-01, 4.2620e-01, 6.9776e-01, 6.8027e-01, 3.5835e-02, 1.8883e-01,\n",
       "        5.6694e-02, 8.9132e-01, 9.2457e-01, 6.6425e-01, 1.0963e-01, 8.9240e-01,\n",
       "        7.2119e-01, 3.2032e-01, 7.8596e-01, 2.2318e-01, 9.2112e-01, 4.9736e-01,\n",
       "        2.1937e-01, 2.0161e-01, 3.7522e-02, 4.4416e-01, 1.5229e-01, 1.9523e-01,\n",
       "        1.4593e-02, 2.7597e-01, 1.2106e-01, 1.2228e-01, 4.9617e-01, 8.6680e-01,\n",
       "        7.9674e-01, 1.7860e-01, 9.5009e-01, 3.9491e-01, 4.3897e-01, 6.8587e-01,\n",
       "        3.8273e-01, 6.7231e-01, 8.0983e-01, 2.2585e-01, 4.5408e-01, 3.2600e-02,\n",
       "        6.6162e-01, 5.8193e-01, 7.2637e-01, 4.3309e-01, 3.0091e-01, 9.5428e-01,\n",
       "        4.0890e-01, 4.5803e-01, 8.1541e-01, 4.6375e-01, 4.9134e-01, 8.8907e-01,\n",
       "        1.0418e-01, 8.0382e-01, 2.6015e-01, 1.8410e-01, 8.2823e-01, 1.0067e-01,\n",
       "        6.3880e-01, 6.7795e-01, 3.6065e-01, 5.7303e-01, 4.2625e-01, 4.7223e-01,\n",
       "        5.5296e-01, 3.0798e-01, 4.9660e-01, 6.3781e-01, 7.3778e-01, 4.8809e-02,\n",
       "        6.9669e-01, 6.1733e-01, 3.8350e-01, 3.2793e-01, 7.8491e-01, 3.3005e-01,\n",
       "        3.9831e-01, 9.6430e-01, 5.1238e-01, 7.1820e-01, 4.7775e-01, 6.0548e-03,\n",
       "        5.1859e-01, 7.8464e-02, 6.5257e-01, 1.3692e-02, 2.9088e-02, 1.2857e-01,\n",
       "        5.5916e-01, 9.3119e-01, 1.1377e-01, 9.3876e-01, 3.9403e-01, 6.0261e-01,\n",
       "        5.3925e-01, 2.8240e-01, 3.1114e-01, 5.1972e-01, 6.8302e-02, 7.5068e-02,\n",
       "        4.7410e-01, 5.6680e-01, 3.6960e-01, 2.5164e-01, 7.8292e-01, 8.6940e-01,\n",
       "        1.9051e-01, 2.0748e-01, 6.8547e-01, 7.0077e-01, 6.5802e-01, 3.2759e-01,\n",
       "        4.8182e-01, 3.0821e-01, 5.6653e-01, 5.2428e-01, 3.0072e-01, 6.4038e-01,\n",
       "        9.2705e-01, 5.7987e-01, 4.9293e-01, 3.0682e-01, 7.5602e-01, 7.3042e-01,\n",
       "        5.7202e-01, 3.4327e-01, 1.9623e-01, 1.5637e-01, 1.3256e-01, 9.7172e-02,\n",
       "        9.5177e-03, 6.7773e-01, 8.7316e-01, 8.9363e-01, 2.2136e-01, 2.0216e-01,\n",
       "        3.1429e-01, 9.6791e-01, 2.3011e-01, 4.0593e-01, 8.7586e-01, 4.1102e-01,\n",
       "        1.5135e-01, 2.2458e-01, 3.4758e-01, 7.1239e-01, 7.9066e-01, 2.2905e-02,\n",
       "        2.4207e-02, 3.0718e-01, 1.6506e-01, 4.9765e-01, 4.8162e-01, 4.6650e-01,\n",
       "        8.3761e-01, 5.5195e-01, 1.8077e-02, 9.9371e-01, 6.0253e-02, 7.5360e-01,\n",
       "        1.3850e-01, 3.5739e-01, 9.1526e-01, 8.5840e-01, 5.7860e-01, 2.9576e-01,\n",
       "        4.9515e-01, 6.8979e-01, 9.3265e-01, 7.4938e-01, 1.2339e-01, 1.3354e-01,\n",
       "        9.5488e-02, 8.8044e-01, 1.1585e-01, 1.2507e-01, 1.6965e-01, 1.6437e-01,\n",
       "        5.6900e-01, 8.4030e-01, 2.6298e-01, 7.7929e-01, 1.3179e-01, 8.3847e-01,\n",
       "        8.2279e-01, 9.2947e-01, 5.6913e-01, 5.8463e-01, 5.8979e-01, 1.7962e-02,\n",
       "        5.9605e-01, 9.1018e-01, 8.7324e-02, 1.3632e-01, 7.8070e-01, 1.8017e-01,\n",
       "        2.9160e-01, 1.7944e-01, 5.0657e-02, 5.2115e-01, 1.5296e-01, 6.9036e-01,\n",
       "        4.5324e-01, 8.5984e-02, 6.5880e-01, 1.5685e-01, 4.6240e-01, 2.6960e-01,\n",
       "        3.3536e-01, 9.9463e-01, 4.9987e-01, 7.7699e-01, 8.1095e-01, 9.6373e-01,\n",
       "        7.9444e-01, 8.0970e-02, 9.9095e-01, 9.3924e-01, 4.8678e-01, 9.9256e-01,\n",
       "        9.8666e-01, 7.0645e-01, 3.6373e-01, 7.9162e-01, 4.9121e-01, 1.9284e-01,\n",
       "        4.9417e-01, 4.7987e-01, 7.9709e-01, 4.6195e-01, 6.2783e-02, 6.1396e-01,\n",
       "        2.9376e-03, 8.9267e-01, 3.0829e-01, 7.8095e-01, 4.5279e-01, 7.0988e-01,\n",
       "        2.1896e-03, 3.9751e-02, 4.3279e-04, 5.4439e-01, 9.0704e-01, 8.2156e-01,\n",
       "        6.6294e-01, 9.2747e-01, 7.9778e-01, 2.8648e-01, 1.4129e-01, 5.2114e-01,\n",
       "        4.2847e-01, 5.1843e-01, 9.2745e-01, 8.1957e-01, 2.4448e-01, 9.6340e-01,\n",
       "        2.5153e-01, 1.6294e-01, 6.4651e-01, 2.5274e-01, 2.3744e-01, 9.0034e-01,\n",
       "        1.0850e-01, 2.3314e-01, 1.1925e-02, 6.7548e-01, 6.0389e-01, 3.1800e-01,\n",
       "        7.3601e-01, 7.5766e-02, 3.1885e-01, 8.0810e-01, 5.4054e-01, 5.6314e-01,\n",
       "        4.1949e-02, 9.1220e-01, 3.5982e-01, 9.0581e-01, 6.1123e-01, 4.9040e-01,\n",
       "        1.4115e-01, 6.5586e-01, 9.3308e-01, 7.3918e-01, 4.4851e-01, 7.0119e-01,\n",
       "        4.1726e-02, 8.6912e-01, 9.8065e-01, 2.8670e-01, 3.1555e-02, 5.4965e-01,\n",
       "        2.4501e-02, 3.1973e-01, 4.7334e-01, 2.3094e-01, 4.6336e-01, 6.7137e-01,\n",
       "        4.6454e-01, 7.9423e-01, 6.8218e-01, 6.3772e-01, 8.5966e-01, 2.1086e-01,\n",
       "        4.7027e-01, 3.9366e-01, 2.4410e-01, 2.7755e-02, 1.5188e-01, 9.9665e-01,\n",
       "        4.1591e-01, 3.4769e-01, 3.9091e-02, 5.6866e-01, 8.4241e-01, 9.6839e-01,\n",
       "        8.1975e-01, 9.7403e-01, 7.8272e-01, 3.7586e-01, 6.2925e-01, 6.8436e-01,\n",
       "        2.3276e-01, 3.7661e-01, 1.9562e-01, 8.6665e-01, 4.9739e-01, 4.1241e-01,\n",
       "        3.8378e-01, 4.0435e-01, 1.1535e-01, 4.5884e-01, 7.3037e-01, 8.7111e-02,\n",
       "        2.5592e-01, 2.1238e-01, 4.1332e-01, 2.0018e-01, 8.9020e-01, 2.0851e-01,\n",
       "        4.7978e-01, 7.4972e-01, 5.0669e-01, 2.7125e-01, 6.6285e-01, 4.8785e-01,\n",
       "        3.0530e-01, 2.0190e-01, 6.0040e-01, 9.4976e-01, 1.7593e-01, 8.6057e-01,\n",
       "        5.9719e-01, 1.0492e-01, 8.2912e-01, 4.3310e-01, 4.1510e-01, 6.8073e-01,\n",
       "        8.2512e-01, 7.1143e-01, 4.1465e-01, 7.0900e-01, 5.8714e-01, 9.6458e-01,\n",
       "        5.0124e-01, 8.2802e-01, 9.9778e-01, 6.1997e-01, 9.7585e-01, 3.9442e-01,\n",
       "        7.4762e-01, 8.2225e-02, 1.6546e-01, 2.7542e-01, 5.9915e-01, 8.8288e-01,\n",
       "        2.5769e-01, 5.4916e-01, 7.5344e-01, 2.6105e-01, 5.3612e-01, 4.2051e-01,\n",
       "        6.3784e-01, 3.4993e-01, 1.5149e-01, 5.7155e-01, 7.8429e-01, 9.4738e-01,\n",
       "        5.5697e-01, 7.0071e-02, 3.3499e-01, 2.6306e-01, 7.9247e-01, 7.6540e-01,\n",
       "        9.6113e-01, 4.0321e-01, 3.8736e-01, 5.7455e-01, 3.1096e-01, 7.6184e-01,\n",
       "        7.6243e-01, 9.2248e-01, 1.3966e-01, 5.8037e-01])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the difference between ```view()``` and ```reshape()``` is that ```view()``` operates as a view on the original tensor, so if the underlying data is changed, the view will change too (and vice versa). However, ```view()``` can throw errors if requiered view is not contiguous; that is, it doesn't share the same block of memory it would occupy if a new tensor of the required shape was created from scratch. If this happens, you have to call ```tensor.contiguous()``` before you can use the ```view()```. However, ```reshape()``` does all that behind the scenes, so in general it is recommended using ```reshape()``` rather than ```view()```.\n",
    "\n",
    "Finally, you might need to rearrange the dimensions of a tensor. You will likely come accross this with images, which often are stored as ```[height, width, channel]``` tensors, but PyTorch prefers to deal with tese in a ```[channel, height, width]```. You can use ```permure()``` to deal with these in a fairly straightforward manner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "hwc_tensor = torch.rand(640, 480, 3) #640x480 RBG Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "chw_tensor = hwc_tensor.permute(2,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 640, 480])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chw_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tensor Broadcasting**\n",
    "\n",
    "Many PyTorch operations support [NumPy Broadcasting Semantics.](https://numpy.org/doc/stable/user/basics.broadcasting.html#module-numpy.doc.broadcasting)\n",
    "\n",
    "In short, if a PyTorch operation supports broadcast (Perform operations between a tensor and smaller tensor), then its Tensor arguments can be automatically expanded to be of equal sizes (without making copies of the data).\n",
    "\n",
    "General semantics\n",
    "Two tensors are “broadcastable” if the following rules hold:\n",
    "\n",
    "Each tensor has at least one dimension.\n",
    "\n",
    "When iterating over the dimension sizes, starting at the trailing dimension, the dimension sizes must either be equal, one of them is 1, or one of them does not exist.\n",
    "\n",
    "In out use of bradcasting, it works because 1 has a dimension of 1, and there are no other dimensions, the 1 can be expanded to cover the other tensor. If we tried to add a ```[2,2]``` tensor to a ```[3,3]``` tensor, we'd get this error message:\n",
    "\n",
    "```\n",
    "The size of tensor a (2) must match the size of\n",
    "tensor b(3) at not-singleton dimension 1\n",
    "```\n",
    "But we could add a ```[1,3]``` tensor to the ```[3,3]``` tensor without any trouble. This is handy because it increases the brevity of code, and is often faster than manually expanding the tensor yourself.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
