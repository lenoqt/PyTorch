{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a simple classifier that can tell the difference between fish and cats iterating over the design and how the model is built to make it more and more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Traditional challenges**\n",
    "\n",
    "Determine a set of rules to differentiate a cat from a fish, this set of rules can be describing that a cat has a tail, or that a fish has different colors or scales, and apply these rules to an image to determine what are we looking at. There are other caveats that have to be considered in the model, for example, what happens if we find a Manx cat? While is clearly a cat it doesn't have a tail.\n",
    "\n",
    "These rules are just going the get more and more complicated to describe all posible scenarios.\n",
    "\n",
    "What we are after is a function that, givem the input of an image, returns a cat or fish.\n",
    "\n",
    "**Data**\n",
    "\n",
    "First, we need data, How much? Depends, the idea for any deep learning technique to work, you need a LOT of data to train the NN is not necessarily true. However, right now we're going to be training from scratch, which often does require access to a large quantity of data. We need a lot of pictures of fish and cats.\n",
    "\n",
    "A standard collection of images used to train neural networks, called **ImageNet** contains 14 million images and 20,000 image categories. It's the the standard that all image classifiers judge themselves against."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and converting data into formatis that are ready for training can often end up being one of the areas in data science that sucks up far too much of time.\n",
    "\n",
    "PyTorch has developed a standard conventions of interacting with data that make it faily consistent to work with, whether you're working with images, text, audio or video.\n",
    "\n",
    "The two main conventions of interacting with data are **datasets**  and **data loaders**. A dataset is a Python class that allows us to get at the data we're supplying to the neural network. A data loader is what feeds data from the dataset into the network.\n",
    "\n",
    "Looking at the following class, every dataset, no matter whether includes images, audio, text, 3D, stock market info, or whatever, can interact iwth PyTorch if it satisfies this abstract Python class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Dataset(object):\n",
    "#     def __getitem__(self, index):\n",
    "#         raise NotImplementedError\n",
    "#     def __len__(self):\n",
    "#         raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is fairly straighforward: we have to implement a method that returns the size of our dataset ```(len)```, and implement a method that can retrieve an item from our dataset in a ```(label, tensor)``` pair. This is called by the data loader as it is pushing data into the neural network for training. So we have the body ```__getitem__``` that can take an image and transform it into a tensor and return that and the label back so PyTorch can operate it. This is fine, but you can imagine that this scenario comes up a lot.\n",
    "\n",
    "**Building a training dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageFile\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = 'C02Dataset/train/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = transforms.Compose([\n",
    "    transforms.Resize((64,64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406],\n",
    "                        std=[0.229,0.224,0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_image(path):\n",
    "    try:\n",
    "        im = Image.open(path)\n",
    "        return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torchvision.datasets.ImageFolder(root=train_data_path, \n",
    "                                              transform=transforms, is_valid_file=check_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```torchvision``` allows to specify a list of transforms that will be applied to an image before it gets into the NN. The default transform is to take image data and turn in into a tensor ```transforms.ToTensor()```, but we also doing a couple of other things that might not seem obvious.\n",
    "\n",
    "GPUs are built to be fast at performing calculations that are a standard size. But we probably have an assortment of images at many resolutions. To increase our processing performance, we scale everything to 64x64 resolution via ```Resize(64)``` transform. After the image is converted into a tensor, we finally normalize tensor around a specific set of mean and standard deviation points.\n",
    "\n",
    "Normalizing is important to avoid a **exploding gradient** problem, in which you keep the values between 0 and 1 during training phase this prevents values from getting too large. These values of $\\sigma$ and $\\mu$ are previously taken from ImageNet, for other implementations you'd have to calculate that $\\sigma$ and $\\mu$.\n",
    "\n",
    "**Building validation and test datasets**\n",
    "\n",
    "The training data is setup, but we need to repeat the same steps for validation and test datasets. What is the difference? One danger in deep learning and all machine learning in fact, is the concept of overfitting when the model gets really good at recognizing what is been trained on but it can generalize to examples out of the training samples. To prevent this, we use a validation set, which is another serie of cats and fishes that do not occur in the training set. At the end of each training cycle ```(epoch)```, we compare against this set to make sure our network isn't getting things wrong. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_path = 'C02Dataset/val/' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = torchvision.datasets.ImageFolder(root=val_data_path,\n",
    "                                           transform=transforms, is_valid_file=check_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_path = 'C02Dataset/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = torchvision.datasets.ImageFolder(root=test_data_path,\n",
    "                                            transform=transforms, is_valid_file=check_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training set:** Used in the training pass to update the model.\n",
    "**Validation set:** Used to evaluate how the model is generalizing to the problem domain, rather than fitting to the training data (this doesn't update the model directly).\n",
    "**Test set:** A final dataset that provides a final evaluation of the model's performance after training is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64 #How many images will go each epoch\n",
    "train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size)\n",
    "val_data_loader  = torch.utils.data.DataLoader(val_data, batch_size=batch_size) \n",
    "test_data_loader  = torch.utils.data.DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building the neural network**\n",
    "\n",
    "Starting with:\n",
    "\n",
    "${Input Layer} \\Leftrightarrow {Hidden_1} \\Leftrightarrow {Output Layer}$\n",
    "\n",
    "Fully connected net with a **ReLU** as activation function ${max}({0},{x})$ so if the input is negative the result is 0. **ReLU** is more appropiate for this kind of binary classification because if we implemented a **Softmax** function this goes from adding from ${0}$ to ${1}$ \"probabilities\" and it would exaggerate the differences, so it is better to use in the hidden layers **ReLU** and **Softmax** at the output layer then use ```argmax()``` when trying to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(12288, 84)\n",
    "        self.fc2 = nn.Linear(84, 50)\n",
    "        self.fc3 = nn.Linear(50,2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 12288)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplenet = SimpleNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At ```__init__``` we do the setup, calling the superclass constructor and 3 fully connected layers this class only implements **inference** it means the data flows through the network and make predictions. First we have to convert the 3D tensor x and y plus three-channel color information **RBG** into a 1D-Tensor so it can be fed into the first Linear layer, we use that with the method ```view()```, from there we apply the layers and the activation for each of it returning the softmax output value that will contain the probability of the image to be a cat or fish.\n",
    "\n",
    "The numbers in the hidden layers are somewhat arbitrary, with the exception of the output that has to match the desired output (2), the data has to be compressed as it goes down the stack so we prevent that the network cheating by only passing the n connections to the n outputs and consider the job done.\n",
    "\n",
    "**Loss Function** \n",
    "\n",
    "For multi-class categorization problems it is recommended to use ```CrossEntropyLoss```. Another loss function is ```MSELoss```, which is recommended when you making a numerical prediction.\n",
    "\n",
    "One thing to be aware is that ```CrossEntropyLoss``` incorporates ```softmax()``` as part of its operation, so we can remove this sofmax activation function from the last layer.\n",
    "\n",
    "**Optimizing**\n",
    "\n",
    "We can optimize using the loss function to determine the difference between the prediction and the actual label, and then use that information to update weights on the net minimizing as much as possible the loss, for this we use an optimizer. One of the most used is **SGD *Stochastic Gradient Descent*** we will go through several optimizers and pick the best suit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(simplenet.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training we make a loop going forward the net, calculate error, backpropagate and update weights, and so on. We make use of ```zero_grad()``` in the loop to make sure that the gradients aren't accumulated as this is a default behaviour, with this we guarantee we only have the gradients for each batch of training for our optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SimpleNet(\n",
       "  (fc1): Linear(in_features=12288, out_features=84, bias=True)\n",
       "  (fc2): Linear(in_features=84, out_features=50, bias=True)\n",
       "  (fc3): Linear(in_features=50, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(device)\n",
    "simplenet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, train_loader, val_loader, epochs=20, device=\"cpu\"):\n",
    "    for epoch in range(1, epochs+1):\n",
    "        training_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            output = model(inputs)\n",
    "            loss = loss_fn(output, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_loss += loss.data.item() * inputs.size(0)\n",
    "        training_loss /= len(train_loader.dataset)\n",
    "        \n",
    "        model.eval()\n",
    "        num_correct = 0 \n",
    "        num_examples = 0\n",
    "        for batch in val_loader:\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.to(device)\n",
    "            output = model(inputs)\n",
    "            targets = targets.to(device)\n",
    "            loss = loss_fn(output,targets) \n",
    "            valid_loss += loss.data.item() * inputs.size(0)\n",
    "            correct = torch.eq(torch.max(F.softmax(output, dim=1), dim=1)[1], targets)\n",
    "            num_correct += torch.sum(correct).item()\n",
    "            num_examples += correct.shape[0]\n",
    "        valid_loss /= len(val_loader.dataset)\n",
    "\n",
    "        print('Epoch: {}, Training Loss: {:.2f}, Validation Loss: {:.2f}, accuracy = {:.2f}'.format(epoch, training_loss,\n",
    "        valid_loss, num_correct / num_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 1.78, Validation Loss: 6.02, accuracy = 0.24\n",
      "Epoch: 2, Training Loss: 2.55, Validation Loss: 1.45, accuracy = 0.46\n",
      "Epoch: 3, Training Loss: 0.79, Validation Loss: 1.17, accuracy = 0.55\n",
      "Epoch: 4, Training Loss: 0.55, Validation Loss: 1.12, accuracy = 0.63\n",
      "Epoch: 5, Training Loss: 0.38, Validation Loss: 1.02, accuracy = 0.62\n",
      "Epoch: 6, Training Loss: 0.30, Validation Loss: 0.99, accuracy = 0.65\n",
      "Epoch: 7, Training Loss: 0.23, Validation Loss: 0.98, accuracy = 0.67\n",
      "Epoch: 8, Training Loss: 0.19, Validation Loss: 1.00, accuracy = 0.64\n",
      "Epoch: 9, Training Loss: 0.17, Validation Loss: 1.01, accuracy = 0.68\n",
      "Epoch: 10, Training Loss: 0.14, Validation Loss: 1.03, accuracy = 0.66\n",
      "Epoch: 11, Training Loss: 0.13, Validation Loss: 1.04, accuracy = 0.68\n",
      "Epoch: 12, Training Loss: 0.10, Validation Loss: 1.08, accuracy = 0.68\n",
      "Epoch: 13, Training Loss: 0.09, Validation Loss: 1.08, accuracy = 0.68\n",
      "Epoch: 14, Training Loss: 0.08, Validation Loss: 1.14, accuracy = 0.66\n",
      "Epoch: 15, Training Loss: 0.07, Validation Loss: 1.14, accuracy = 0.66\n",
      "Epoch: 16, Training Loss: 0.06, Validation Loss: 1.18, accuracy = 0.66\n",
      "Epoch: 17, Training Loss: 0.05, Validation Loss: 1.19, accuracy = 0.64\n",
      "Epoch: 18, Training Loss: 0.04, Validation Loss: 1.23, accuracy = 0.63\n",
      "Epoch: 19, Training Loss: 0.04, Validation Loss: 1.25, accuracy = 0.63\n",
      "Epoch: 20, Training Loss: 0.03, Validation Loss: 1.23, accuracy = 0.65\n"
     ]
    }
   ],
   "source": [
    "train(simplenet, optimizer,torch.nn.CrossEntropyLoss(), train_data_loader,val_data_loader, epochs=20, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['cat','fish']\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "cats = [f for f in listdir('C02Dataset/val/cat') if isfile(join('C02Dataset/val/cat', f))]\n",
    "fishes = [f for f in listdir('C02Dataset/val/fish') if isfile(join('C02Dataset/val/fish', f))]\n",
    "cats_pred = []\n",
    "fishes_pred = []\n",
    "for cat in cats:\n",
    "    img = Image.open(\"C02Dataset/val/cat/\"+cat) \n",
    "    img = transforms(img).to(device)\n",
    "    img = torch.unsqueeze(img, 0)\n",
    "    simplenet.eval()\n",
    "    prediction = F.softmax(simplenet(img), dim=1)\n",
    "    prediction = prediction.argmax()\n",
    "    cats_pred.append(labels[prediction])\n",
    "for fish in fishes:\n",
    "    img = Image.open(\"C02Dataset/val/fish/\"+fish) \n",
    "    img = transforms(img).to(device)\n",
    "    img = torch.unsqueeze(img, 0)\n",
    "    simplenet.eval()\n",
    "    prediction = F.softmax(simplenet(img), dim=1)\n",
    "    prediction = prediction.argmax()\n",
    "    fishes_pred.append(labels[prediction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_samples = len(cats_pred) + len(fishes_pred)\n",
    "true_positives = sum(1 for i in cats_pred if i == 'cat') \n",
    "false_negative = sum(1 for i in cats_pred if i == 'fish')\n",
    "false_positives = sum(1 for i in fishes_pred if i == 'cat')\n",
    "true_negative = sum(1 for i in fishes_pred if i == 'fish')\n",
    "classification_accuracy = true_positives+true_negative/total_samples*100\n",
    "prevelence = len(cats_pred)/total_samples\n",
    "PPV = true_positives/true_positives+true_negative\n",
    "FDR = false_positives/true_positives+true_negative\n",
    "FOR = false_negative/false_negative\n",
    "error_rate = (1 - (true_positives/total_samples))*100\n",
    "x = torch.tensor([[true_positives, false_positives], [false_negative, true_negative]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7927927927927928"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prevelence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats_expected = torch.ones(len(cats), dtype=torch.int8).tolist()\n",
    "cats_predicted = [1 if x == 'cat' else 0 for x in cats_pred]\n",
    "fishes_expected = torch.zeros(len(fishes), dtype=torch.int8).tolist()\n",
    "fishes_predicted = [1 if x == 'cat' else 0 for x in fishes_pred]\n",
    "expected = cats_expected + fishes_expected\n",
    "predicted = cats_predicted + fishes_predicted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20  3]\n",
      " [36 52]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(expected, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[52,  3],\n",
       "        [36, 20]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70.01801801801801"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Manually calculate val_loss and ROI analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving Models\n",
    "We can either save the entire model using save or just the parameters using state_dict. Using the latter is normally preferable, as it allows you to reuse parameters even if the model's structure changes (or apply parameters from one model to another)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(simplenet, \"simplenet\") \n",
    "simplenet = torch.load(\"simplenet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "torch.save(simplenet.state_dict(), \"simplenet\")    \n",
    "simplenet = SimpleNet()\n",
    "simplenet_state_dict = torch.load(\"simplenet\")\n",
    "simplenet.load_state_dict(simplenet_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
